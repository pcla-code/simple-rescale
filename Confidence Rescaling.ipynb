{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Take detector confidences that do not aggregate to original category proportions\n",
    "# and rescale them (very simple constant addition scaling) to original category proportions\n",
    "\n",
    "# Use: a set of detectors developed with SMOTE or other rebalancing algorithms will no longer\n",
    "# match original proportions of categories, and will be scaled differently from each other.\n",
    "# This code provides offsets that can be added to confidences to make them comparable.\n",
    "\n",
    "# Instructions: Run each cell of this Jupyter Notebook one after the other\n",
    "\n",
    "# Input: A file with confidences for each data point for each category, \n",
    "# and a file with original proportions for each category\n",
    "# Output: Rescaled confidences, final proportionsS\n",
    "\n",
    "# By Ryan S. Baker, 7/11/2024\n",
    "# MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cb575ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_proportions(df_orig):\n",
    "    \"\"\"\n",
    "    Extends the DataFrame by determining which column has the highest value for each row,\n",
    "    appending this column name, counting occurrences, and calculating the proportion of these occurrences.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to process, where rows represent variables and columns represent different measures.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame named 'proportiontable' that contains the proportions of each column being the MaxColumn.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_orig.copy()\n",
    "\n",
    "    # Determine the column name with the highest value for each row\n",
    "    df['MaxColumn'] = df.idxmax(axis=1)\n",
    "    \n",
    "    # Count occurrences of each column being the maximum\n",
    "    counts = df['MaxColumn'].value_counts()\n",
    "    \n",
    "    # Calculate the proportion of each column being the maximum\n",
    "    total_rows = len(df)\n",
    "    proportions = counts / total_rows\n",
    "    \n",
    "    # Create the proportion table including all original columns\n",
    "    proportion_table = pd.DataFrame(index=df.columns[:-1])  # Exclude 'MaxColumn' from the index\n",
    "    proportion_table['Proportion'] = proportions\n",
    "    proportion_table['Proportion'].fillna(0, inplace=True)  # Fill missing values with 0\n",
    "    \n",
    "    return proportion_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "77aac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_offsets_to_dataframe(df_orig, offsets):\n",
    "    \"\"\"\n",
    "    Applies offsets to each column of the DataFrame based on the provided dictionary of offsets.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame to process, where rows represent variables and columns have specific labels.\n",
    "    offsets (dict): A dictionary where keys correspond to DataFrame column labels and values are the offsets to apply.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with offsets applied to the original values.\n",
    "    \"\"\"\n",
    "    df = df_orig.copy()\n",
    "    # Ensure all specified offsets are present in the DataFrame columns\n",
    "    for column in df.columns:\n",
    "        if column in offsets:\n",
    "            df[column] = df[column] + offsets[column]\n",
    "        else:\n",
    "            if column != 'MaxColumn':\n",
    "                print(f\"No offset provided for column {column}, leaving unchanged.\")\n",
    "   # print (df)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "608e0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tables(table1, table2):\n",
    "    \"\"\"\n",
    "    Compares two tables by subtracting the values of matching labels from each other.\n",
    "    \n",
    "    Args:\n",
    "    table1 (DataFrame): The first DataFrame with columns 'Label' and 'Value'.\n",
    "    table2 (DataFrame): The second DataFrame with columns 'Label' and 'Value'.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with labels and the difference in values between the two tables.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the difference between the two tables\n",
    "    result = table1['Proportion'].subtract(table2['Proportion'], fill_value=0).reset_index()\n",
    "    result.columns = ['Category', 'Difference']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "972fa3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_abs_diff_tables(table1, table2):\n",
    "    \"\"\"\n",
    "    Compares two tables by subtracting the values of matching labels from each other,\n",
    "    taking the absolute value of these differences, and summing them up.\n",
    "    \n",
    "    Args:\n",
    "    table1 (DataFrame): The first DataFrame with columns 'Label' and 'Value'.\n",
    "    table2 (DataFrame): The second DataFrame with columns 'Label' and 'Value'.\n",
    "    \n",
    "    Returns:\n",
    "    float: The sum of the absolute differences between the two tables.\n",
    "    \n",
    "    Absolute differences used rather than squared differences to avoid focusing \n",
    "    fit on the majority class. In practice, will make limited difference and could\n",
    "    easily be substituted\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the absolute difference between the two tables\n",
    "    result = table1['Proportion'].subtract(table2['Proportion'], fill_value=0).abs()\n",
    "\n",
    "    # Sum up all absolute differences\n",
    "    total_difference = result.sum()\n",
    "    \n",
    "    return total_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b1a49fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_table_diffs(confidences, original_prop, offsets):\n",
    "  \n",
    "    # Get column names and row labels\n",
    "    conf_col_names = confidences.columns\n",
    "    orig_row_labels = original_prop.index\n",
    "    \n",
    "    \n",
    "    # Check for columns in confidences not in original_prop rows\n",
    "    missing_in_orig = set(conf_col_names) - set(orig_row_labels)\n",
    "    if missing_in_orig:\n",
    "        print(\"Error: The following columns in 'confidences-dwd.csv' do not appear as rows in 'original-prop-dwd.csv':\", missing_in_orig)\n",
    "    \n",
    "    # Check for rows in original_prop not in confidences columns\n",
    "    missing_in_conf = set(orig_row_labels) - set(conf_col_names)\n",
    "    if missing_in_conf:\n",
    "        print(\"Error: The following rows in 'original-prop-dwd.csv' do not appear as columns in 'confidences-dwd.csv':\", missing_in_conf)\n",
    "   \n",
    "    category_proportions = calculate_category_proportions(confidences)\n",
    "    \n",
    "    #print(category_proportions)\n",
    "    \n",
    "    # Run the function\n",
    "    modified_df = apply_offsets_to_dataframe(confidences, offsets)\n",
    "    \n",
    "    modified_category_proportions = calculate_category_proportions(modified_df)\n",
    "    \n",
    "    #print(modified_category_proportions)\n",
    "    #print(original_prop)\n",
    "    \n",
    "    difftable = compare_tables(original_prop,modified_category_proportions)\n",
    "    \n",
    "    #print (difftable)\n",
    "    \n",
    "    sum_abs_diff = sum_abs_diff_tables(original_prop,modified_category_proportions)\n",
    "    \n",
    "    return sum_abs_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3ab29bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def make_offsets(confidences, x):\n",
    "    \"\"\"Create a dictionary of offsets from the optimization variables.\"\"\"\n",
    "    return {column: x[i] for i, column in enumerate(confidences.columns)}\n",
    "\n",
    "def objective(x, confidences, original_prop):\n",
    "    \"\"\"Objective function to be minimized.\"\"\"\n",
    "    offsets = make_offsets(confidences, x)\n",
    "    #print(offsets)\n",
    "    res = get_table_diffs(confidences, original_prop, offsets)\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "def fit_offsets(confidence_filename, original_prop_filename):\n",
    "    # Load the CSV files\n",
    "    confidences = pd.read_csv(confidence_filename)\n",
    "    original_prop = pd.read_csv(original_prop_filename, index_col=0)  # Assuming the first column is the index\n",
    "        \n",
    "    # Initial guess for offsets (all zeros)\n",
    "    initial_guess = [0.0] * len(confidences.columns)\n",
    "\n",
    "    # Setting initial step sizes for each dimension\n",
    "    initial_simplex = [initial_guess]\n",
    "    initial_step_size = 0.1  # Step size of 0.1 for each dimension\n",
    "    for i in range(len(initial_guess)):\n",
    "        simplex_point = initial_guess.copy()\n",
    "        simplex_point[i] += initial_step_size\n",
    "        initial_simplex.append(simplex_point)\n",
    "    \n",
    "    # Perform minimization using the Nelder-Mead method\n",
    "    result = minimize(objective, initial_guess, args=(confidences, original_prop), method='Nelder-Mead', options={'initial_simplex': initial_simplex, 'xatol': 0.001, 'fatol': 0.001, 'maxiter': 10000})\n",
    "\n",
    "    # Extract the best offsets found\n",
    "    best_offsets = make_offsets(confidences, result.x)\n",
    "    \n",
    "    # Print the best offsets and the resulting diff\n",
    "    print(\"Best Offsets:\", best_offsets)\n",
    "    diff = get_table_diffs(confidences, original_prop, best_offsets)\n",
    "    \n",
    "    modified_df = apply_offsets_to_dataframe(confidences, best_offsets)\n",
    "        \n",
    "    modified_category_proportions = calculate_category_proportions(modified_df)\n",
    "    \n",
    "    print(modified_category_proportions)\n",
    "    print()\n",
    "    \n",
    "    print(\"Minimum Difference:\", diff)\n",
    "    \n",
    "    return modified_df\n",
    "\n",
    "    # modified_df contains the actual rescaled detector confidences for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "73ef78b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Offsets: {'ConcentrationBROMP': 0.2556039080825493, 'ConfusionBROMP': 0.04017999129957831, 'BoredomBROMP': -0.09996964472004519, 'FrustrationBROMP': -0.0008127388961018402, 'DelightBROMP': -0.14630263091234566}\n",
      "                    Proportion\n",
      "ConcentrationBROMP    0.822303\n",
      "ConfusionBROMP        0.065773\n",
      "BoredomBROMP          0.047312\n",
      "FrustrationBROMP      0.051260\n",
      "DelightBROMP          0.013352\n",
      "\n",
      "Minimum Difference: 0.0002673859692325798\n",
      "       ConcentrationBROMP  ConfusionBROMP  BoredomBROMP  FrustrationBROMP  \\\n",
      "0                0.732189        0.593115      0.222694          0.367793   \n",
      "1                0.763120        0.593115      0.143797          0.368300   \n",
      "2                0.902952        0.426395      0.133699          0.368534   \n",
      "3                0.881862        0.451860      0.189924          0.426728   \n",
      "4                0.894440        0.446724      0.156182          0.414395   \n",
      "...                   ...             ...           ...               ...   \n",
      "17221            0.890410        0.696110      0.099709          0.610708   \n",
      "17222            0.846396        0.694502      0.182416          0.590025   \n",
      "17223            0.866843        0.563336      0.123042          0.637334   \n",
      "17224            0.854248        0.447735      0.140427          0.691883   \n",
      "17225            0.874901        0.447735      0.093895          0.691883   \n",
      "\n",
      "       DelightBROMP  \n",
      "0          0.184675  \n",
      "1          0.184675  \n",
      "2          0.186445  \n",
      "3          0.186445  \n",
      "4          0.189839  \n",
      "...             ...  \n",
      "17221      0.118730  \n",
      "17222      0.128277  \n",
      "17223      0.131417  \n",
      "17224      0.136022  \n",
      "17225      0.136022  \n",
      "\n",
      "[17226 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# File names (you will need to change the pathname to match your computer)\n",
    "confidence_filename = 'C:/july2024/confidences-dwd.csv'\n",
    "original_prop_filename = 'C:/july2024/original-prop-dwd.csv'\n",
    "\n",
    "# Run the function\n",
    "modified_df = fit_offsets (confidence_filename,original_prop_filename)\n",
    "\n",
    "# print(modified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba0f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e9259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
